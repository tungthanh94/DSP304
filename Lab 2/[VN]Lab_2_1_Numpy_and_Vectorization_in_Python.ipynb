{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"XHpfv","launcher_item_id":"Zh0CU"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"[VN]Lab_2_1_Numpy_and_Vectorization_in_Python.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"WdRg6klznfvQ"},"source":["Lab này sẽ giới thiệu ngắn gọn về Python. Ngay cả khi bạn đã sử dụng Python trước đó, điều này sẽ giúp bạn làm quen với các hàm mà chúng ta sẽ cần.\n","\n","**Hướng dẫn:**\n","- Bạn sẽ sử dụng Python 3.\n","- Tránh sử dụng vòng lặp for và vòng lặp while, trừ khi bạn được yêu cầu rõ ràng làm như vậy.\n","- Sau khi mã hóa hàm của bạn, hãy chạy cell ngay bên dưới để kiểm tra xem kết quả của bạn có đúng không.\n","\n","**Sau lab này, bạn sẽ:**\n","- Có thể sử dụng iPython Notebook\n","- Có thể dùng các hàm numpy và các phép toán ma trận/vectơ numpy\n","- Hiểu được khái niệm \"broadcasting\"\n","- Có thể vectơ hóa code\n","\n","Hãy bắt đầu nào!"]},{"cell_type":"markdown","metadata":{"id":"P811HWD8nfvS"},"source":["## Về iPython Notebook ##\n","\n","iPython Notebooks là môi trường mã hóa tương tác được nhúng trong một trang web. Bạn sẽ sử dụng iPython notebook trong lab này. Bạn chỉ cần viết code giữa chú thích ### START CODE HERE ### và ### END CODE HERE ###. Sau khi viết code, bạn có thể chạy cell bằng cách nhấn \"SHIFT\"+\"ENTER\" hoặc nhấp vào \"Run Cell\" (được biểu thị bằng biểu tượng) ở thanh trên của notebook.\n","\n","Chúng ta thường chỉ định \"(≈ X dòng code)\" trong các chú thích để cho bạn biết bạn cần viết bao nhiêu code. Đó chỉ là một ước tính sơ bộ, vì vậy đừng cảm thấy tồi tệ nếu code của bạn dài hoặc ngắn hơn.\n","\n","**Task 1**: Đặt test thành `\"Hello World\"` trong cell bên dưới để in \"Hello World\" và chạy hai cell bên dưới."]},{"cell_type":"code","metadata":{"id":"r1Njdj-0nfvS"},"source":["### START CODE HERE ### (≈ 1 line of code)\n","\n","### END CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QxH1MobmnfvW","outputId":"e769f6c3-7d81-44a2-9a01-11baa5189e4a"},"source":["print (\"test: \" + test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test: Hello World\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8XIynl-xnfva"},"source":["**Kỳ vọng đầu ra**:\n","test: Hello World"]},{"cell_type":"markdown","metadata":{"id":"CHAU7ijRnfvb"},"source":["<font color='blue'>\n","**Hãy nhớ**:\n","- Chạy cell bằng cách nhấn SHIFT+ENTER (hoặc \"Run cell\")\n","- Chỉ viết code trong các vùng chỉ định dùng Python 3\n","- Không sửa đổi code bên ngoài vùng chỉ định"]},{"cell_type":"markdown","metadata":{"id":"agmiLlBxnfvb"},"source":["## 1 - Tạo các hàm cơ bản với numpy ##\n","\n","Numpy là gói chính cho tính toán khoa học trong Python. Nó được duy trì bởi một cộng đồng lớn (www.numpy.org). Trong lab này, bạn sẽ học một số hàm numpy chính như np.exp, np.log và np.reshape. Bạn sẽ cần biết cách sử dụng các hàm này cho các bài tập trong tương lai.\n","\n","### 1.1 - sigmoid function, np.exp() ###\n","\n","Trước khi sử dụng np.exp(), bạn sẽ sử dụng math.exp() để triển khai hàm sigmoid. Sau đó, bạn hiểu tại sao np.exp() lại thích hợp hơn math.exp().\n","\n","**Task 2**: Xây dựng một hàm trả về giá trị sigmoid của số thực x. Sử dụng math.exp(x) cho hàm mũ.\n","\n","**Nhắc nhở**:\n","$sigmoid (x) = \\frac{1}{1+e^ {- x}}$ đôi khi còn được gọi là hàm logistic. Đây là một hàm phi tuyến tính không chỉ được sử dụng trong Học máy (Hồi quy logistic) mà còn trong Học sâu (Deep Learning).\n","\n","<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n","\n","Để tham chiếu đến một hàm thuộc một gói cụ thể, bạn có thể gọi nó bằng cách sử dụng package_name.functions(). Chạy đoạn code dưới đây để xem ví dụ với math.exp()."]},{"cell_type":"code","metadata":{"id":"H6IExurPnfvc"},"source":["import math\n","\n","def basic_sigmoid(x):\n","    \"\"\"\n","    Compute sigmoid of x.\n","\n","    Arguments:\n","    x -- A scalar\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    \n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6ehIoQMnfvf","outputId":"f1363ad3-d482-4b2b-f7f9-161c988cd89c"},"source":["basic_sigmoid(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9525741268224334"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"x600oAoInfvi"},"source":["**Kỳ vọng đầu ra**: \n","<table style = \"width:40%\">\n","    <tr>\n","    <td>** basic_sigmoid(3) **</td> \n","        <td>0.9525741268224334 </td> \n","    </tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2LEJQT0anfvj"},"source":["Trên thực tế, chúng ta hiếm khi sử dụng thư viện \"math\" trong học sâu vì đầu vào của các hàm là số thực. Trong học sâu, chúng ta chủ yếu sử dụng ma trận và vectơ. Đây là lý do tại sao numpy hữu ích hơn."]},{"cell_type":"code","metadata":{"id":"Voeik2Rwnfvj","outputId":"2ab5dfdc-bbaa-4198-fbdf-91bbbdb22993"},"source":["### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n","x = [1, 2, 3]\n","basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"bad operand type for unary -: 'list'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-5-8ccefa5bf989>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbasic_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# you will see this give an error when you run it, because x is a vector.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-3-a94180fb1694>\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m### START CODE HERE ### (≈ 1 line of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"]}]},{"cell_type":"markdown","metadata":{"id":"yGKg7bjHnfvm"},"source":["Thực tế, nếu $ x = (x_1, x_2, ..., x_n) $ là một vectơ hàng thì $ np.exp (x) $ sẽ áp dụng hàm mũ cho mọi phần tử của x. Do đó, kết quả đầu ra sẽ là: $ np.exp (x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"]},{"cell_type":"code","metadata":{"id":"cTuqrXTVnfvn","outputId":"791e0b35-941e-42ac-e558-3f64ec23f725"},"source":["import numpy as np\n","\n","# example of np.exp\n","x = np.array([1, 2, 3])\n","print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 2.71828183  7.3890561  20.08553692]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AZWQCeu8nfvq"},"source":["Hơn nữa, nếu x là một vectơ thì phép toán Python như $ s = x + 3 $ hoặc $ s = \\frac {1} {x}$ sẽ xuất s dưới dạng một vectơ có cùng kích thước với x."]},{"cell_type":"code","metadata":{"id":"pd5HK3I5nfvq","outputId":"a7e0832b-b562-4e23-eff0-6e4ff4ffde5f"},"source":["# example of vector operation\n","x = np.array([1, 2, 3])\n","print (x + 3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[4 5 6]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U5gjzCGqnfvs"},"source":["Bất kỳ lúc nào bạn cần thêm thông tin về hàm numpy, hãy xem [tài liệu chính thức](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n","\n","Bạn cũng có thể tạo một cell mới trong notebook và viết `np.exp?` (chẳng hạn) để truy cập nhanh vào tài liệu.\n","\n","**Task 3**: Thực thi hàm sigmoid dùng numpy. \n","\n","**Hướng dẫn**: x bây giờ có thể là một số thực, một vectơ hoặc một ma trận. Các cấu trúc dữ liệu chúng ta sử dụng trong numpy biểu diễn các dạng này (vectơ, ma trận,...) được gọi là mảng numpy. Bạn không cần phải biết thêm vào lúc này.\n","$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n","    x_1  \\\\\n","    x_2  \\\\\n","    ...  \\\\\n","    x_n  \\\\\n","\\end{pmatrix} = \\begin{pmatrix}\n","    \\frac{1}{1+e^{-x_1}}  \\\\\n","    \\frac{1}{1+e^{-x_2}}  \\\\\n","    ...  \\\\\n","    \\frac{1}{1+e^{-x_n}}  \\\\\n","\\end{pmatrix}\\tag{1} $$"]},{"cell_type":"code","metadata":{"id":"_6VTga3cnfvt"},"source":["import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n","\n","def sigmoid(x):\n","    \"\"\"\n","    Compute the sigmoid of x\n","\n","    Arguments:\n","    x -- A scalar or numpy array of any size\n","\n","    Return:\n","    s -- sigmoid(x)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    \n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TKZVsrFnfvz","outputId":"dbcbc0e5-d8ff-4d05-a3b8-7ccb0867f81a"},"source":["x = np.array([1, 2, 3])\n","sigmoid(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.73105858, 0.88079708, 0.95257413])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"RoFcMN3Rnfv2"},"source":["**Kỳ vọng đầu ra**: \n","<table>\n","    <tr> \n","        <td> **sigmoid([1,2,3])**</td> \n","        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n","    </tr>\n","</table> \n"]},{"cell_type":"markdown","metadata":{"id":"nRhiWuq8nfv3"},"source":["### 1.2 - Sigmoid gradient\n","\n","Như đã thấy trong bài giảng, bạn sẽ cần phải tính gradient để tối ưu hóa các hàm mất mát bằng cách sử dụng phương pháp truyền ngược. Hãy mã hóa hàm gradient đầu tiên của bạn.\n","\n","**Task 4**: Triển khai hàm sigmoid_grad() để tính gradient của hàm sigmoid liên quan đến đầu vào x của nó. Công thức là:$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n","Bạn thường viết code hàm này theo hai bước:\n","1. Đặt s là sigmoid của x. Bạn sẽ thấy hàm sigmoid(x) của mình hữu ích.\n","2. Tính $\\sigma'(x) = s(1-s)$"]},{"cell_type":"code","metadata":{"id":"1wh9LX7vnfv4"},"source":["def sigmoid_derivative(x):\n","    \"\"\"\n","    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n","    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n","    \n","    Arguments:\n","    x -- A scalar or numpy array\n","\n","    Return:\n","    ds -- Your computed gradient.\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    \n","    \n","    ### END CODE HERE ###\n","    \n","    return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RWS_tgLJnfv6","outputId":"c1006609-410d-4971-fb47-65652e2cfd78"},"source":["x = np.array([1, 2, 3])\n","print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5MtfW1bcnfv9"},"source":["**Kỳ vọng đầu ra**: \n","\n","\n","<table>\n","    <tr> \n","        <td> **sigmoid_derivative([1,2,3])**</td> \n","        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n","    </tr>\n","</table> \n","\n"]},{"cell_type":"markdown","metadata":{"id":"FZJQAXpvnfv-"},"source":["### 1.3 - Định hình lại mảng ###\n","\n","Hai hàm numpy phổ biến được sử dụng trong học sâu là [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n","- X.shape được sử dụng để lấy shape (kích thước) của ma trận/vectơ X.\n","- X.reshape(...) được dùng để định hình lại X sang một số kích thước khác.\n","\n","Ví dụ, trong khoa học máy tính, hình ảnh được biểu diễn bằng mảng 3D có shape $(length, height, depth = 3)$. Tuy nhiên, khi đọc hình ảnh dưới dạng đầu vào của một thuật toán, bạn sẽ chuyển đổi nó thành một vectơ có shape $(length*height*3, 1)$. Nói cách khác, bạn \"triển khai\" hoặc định hình lại mảng 3D thành vectơ 1D.\n","\n","<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n","\n","**Task 5**: Triển khai `image2vector()` nhận đầu vào của shape (length, height, 3) và trả về một vectơ có shape (length\\*height\\ * 3,1). Ví dụ: nếu bạn muốn định hình lại mảng v có shape (a, b, c) thành một vectơ có shape (a * b, c), bạn sẽ thực hiện:\n","``` python\n","v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n","```\n","- Vui lòng không mã hóa các kích thước của hình ảnh như một hằng số. Thay vào đó, hãy tra cứu số lượng bạn cần với `image.shape[0]`, v.v."]},{"cell_type":"code","metadata":{"id":"e9wy3hBanfv-"},"source":["def image2vector(image):\n","    \"\"\"\n","    Argument:\n","    image -- a numpy array of shape (length, height, depth)\n","    \n","    Returns:\n","    v -- a vector of shape (length*height*depth, 1)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    \n","    ### END CODE HERE ###\n","    \n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"18cpElsRnfwA","outputId":"7212d34c-74bb-4508-f821-4865b0cf5491"},"source":["# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n","image = np.array([[[ 0.67826139,  0.29380381],\n","        [ 0.90714982,  0.52835647],\n","        [ 0.4215251 ,  0.45017551]],\n","\n","       [[ 0.92814219,  0.96677647],\n","        [ 0.85304703,  0.52351845],\n","        [ 0.19981397,  0.27417313]],\n","\n","       [[ 0.60659855,  0.00533165],\n","        [ 0.10820313,  0.49978937],\n","        [ 0.34144279,  0.94630077]]])\n","\n","print (\"image2vector(image) = \" + str(image2vector(image)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["image2vector(image) = [[0.67826139]\n"," [0.29380381]\n"," [0.90714982]\n"," [0.52835647]\n"," [0.4215251 ]\n"," [0.45017551]\n"," [0.92814219]\n"," [0.96677647]\n"," [0.85304703]\n"," [0.52351845]\n"," [0.19981397]\n"," [0.27417313]\n"," [0.60659855]\n"," [0.00533165]\n"," [0.10820313]\n"," [0.49978937]\n"," [0.34144279]\n"," [0.94630077]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IZEb9aP7nfwD"},"source":["**Kỳ vọng đầu ra**: \n","\n","\n","<table style=\"width:100%\">\n","     <tr> \n","       <td> **image2vector(image)** </td> \n","       <td> [[ 0.67826139]\n"," [ 0.29380381]\n"," [ 0.90714982]\n"," [ 0.52835647]\n"," [ 0.4215251 ]\n"," [ 0.45017551]\n"," [ 0.92814219]\n"," [ 0.96677647]\n"," [ 0.85304703]\n"," [ 0.52351845]\n"," [ 0.19981397]\n"," [ 0.27417313]\n"," [ 0.60659855]\n"," [ 0.00533165]\n"," [ 0.10820313]\n"," [ 0.49978937]\n"," [ 0.34144279]\n"," [ 0.94630077]]</td> \n","     </tr>\n","    \n","   \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"WdxB2sX5nfwD"},"source":["### 1.4 - Chuẩn hóa các hàng\n","\n","Một kỹ thuật phổ biến khác mà chúng ta sử dụng trong Machine Learning và Deep Learning là chuẩn hóa dữ liệu. Nó thường dẫn đến hiệu suất tốt hơn bởi vì gradient descent hội tụ nhanh hơn sau khi chuẩn hóa. Ở đây, theo cách chuẩn hóa, chúng ta muốn thay đổi x thành $\\frac {x} {\\| x \\|}$ (chia mỗi vectơ hàng của x cho chuẩn của nó).\n","\n","Ví dụ: nếu $$x = \n","\\begin{bmatrix}\n","    0 & 3 & 4 \\\\\n","    2 & 6 & 4 \\\\\n","\\end{bmatrix}\\tag{3}$$ thì $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n","    5 \\\\\n","    \\sqrt{56} \\\\\n","\\end{bmatrix}\\tag{4} $$và        $$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n","    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n","    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n","\\end{bmatrix}\\tag{5}$$ Lưu ý rằng bạn có thể chia các ma trận có kích thước khác nhau và nó vẫn hoạt động tốt: đây được gọi là broadcasting và bạn sẽ tìm hiểu về nó trong phần 5.\n","\n","\n","**Task 6**: Triển khai normalizeRows() để chuẩn hóa các hàng của ma trận. Sau khi áp dụng hàm này cho ma trận đầu vào x, mỗi hàng của x phải là một vectơ có độ dài đơn vị (nghĩa là độ dài 1)."]},{"cell_type":"code","metadata":{"id":"cD-oUbtznfwE"},"source":["def normalizeRows(x):\n","    \"\"\"\n","    Implement a function that normalizes each row of the matrix x (to have unit length).\n","    \n","    Argument:\n","    x -- A numpy matrix of shape (n, m)\n","    \n","    Returns:\n","    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 2 lines of code)\n","    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n","    \n","    \n","    # Divide x by its norm.\n","    x = x/x_norm\n","    ### END CODE HERE ###\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYD8tSVVnfwF","outputId":"53999fd8-d1de-41aa-f200-7d96777dcc8a"},"source":["x = np.array([\n","    [0, 3, 4],\n","    [1, 6, 4]])\n","print(\"normalizeRows(x) = \" + str(normalizeRows(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x shape= (2, 3)\n","x_norm shape= (2, 1)\n","normalizeRows(x) = [[0.         0.6        0.8       ]\n"," [0.13736056 0.82416338 0.54944226]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ipsZBoTLnfwI"},"source":["**Kỳ vọng đầu ra**: \n","\n","<table style=\"width:60%\">\n","\n","     <tr> \n","       <td> **normalizeRows(x)** </td> \n","       <td> [[ 0.          0.6         0.8       ]\n"," [ 0.13736056  0.82416338  0.54944226]]</td> \n","     </tr>\n","    \n","   \n","</table>"]},{"cell_type":"markdown","metadata":{"id":"TGsQ2dhVnfwJ"},"source":["**Lưu ý**:\n","Trong normalizeRows(), bạn có thể thử in các shape của x_norm và x, sau đó chạy lại đánh giá. Bạn sẽ nhận ra rằng chúng có các hình dạng khác nhau. Điều này là bình thường khi x_norm lấy chuẩn của mỗi hàng x. Vì vậy, x_norm có cùng số hàng nhưng chỉ có 1 cột. Vậy nó hoạt động như thế nào khi bạn chia x cho x_norm? Đây được gọi là broadcasting và chúng ta sẽ nói về nó ngay bây giờ!"]},{"cell_type":"markdown","metadata":{"id":"yn45aHHXnfwJ"},"source":["### 1.5 - Broadcasting (Truyền phát) và hàm softmax ####\n","Một khái niệm rất quan trọng cần hiểu trong numpy là \"broadcasting\" (truyền phát). Nó rất hữu ích khi thực hiện các phép toán giữa các mảng có hình dạng khác nhau. Để biết chi tiết đầy đủ về broadcasting, bạn có thể đọc [tài liệu broadcasting] chính thức (http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."]},{"cell_type":"markdown","metadata":{"id":"WQOjOT04nfwJ"},"source":["**Task 7**: Thực hiện một hàm softmax bằng cách sử dụng numpy. Bạn có thể coi softmax như một hàm chuẩn hóa được sử dụng khi thuật toán của bạn cần phân loại hai hoặc nhiều lớp. Bạn sẽ tìm hiểu thêm về softmax trong các bài học tiếp theo.\n","\n","**Hướng dẫn**:\n","- $ \\text{cho } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n","    x_1  &&\n","    x_2 &&\n","    ...  &&\n","    x_n  \n","\\end{bmatrix}) = \\begin{bmatrix}\n","     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n","    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n","    ...  &&\n","    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n","\\end{bmatrix} $ \n","\n","- $\\text{cho ma trận } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ ánh xạ tới phần tử trong hàng thứ $i^{th}$ và cột thứ $j^{th}$ của $x$, ta được: }$  $$softmax(x) = softmax\\begin{bmatrix}\n","    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n","    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n","\\end{bmatrix} = \\begin{bmatrix}\n","    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n","    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n","\\end{bmatrix} = \\begin{pmatrix}\n","    softmax\\text{(hàng thứ nhất của x)}  \\\\\n","    softmax\\text{(hàng thứ hai của x)} \\\\\n","    ...  \\\\\n","    softmax\\text{(hàng cuối của x)} \\\\\n","\\end{pmatrix} $$"]},{"cell_type":"code","metadata":{"id":"OCHr_d6JnfwK"},"source":["def softmax(x):\n","    \"\"\"Calculates the softmax for each row of the input x.\n","\n","    Your code should work for a row vector and also for matrices of shape (n, m).\n","\n","    Argument:\n","    x -- A numpy matrix of shape (n,m)\n","\n","    Returns:\n","    s -- A numpy matrix equal to the softmax of x, of shape (n,m)\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 3 lines of code)\n","    # Apply exp() element-wise to x. Use np.exp(...).\n","    \n","\n","\n","    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n","    \n","    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n","\n","    ### END CODE HERE ###\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5mEZP7HnfwL","outputId":"48ed6bd8-78bf-49a6-b502-c9f47d7985f8"},"source":["x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0]])\n","print(\"softmax(x) = \" + str(softmax(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x shape= (2, 5)\n","x_exp shape= (2, 5)\n","x_sum shape= (2, 1)\n","s shape= (2, 5)\n","softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n","  1.21052389e-04]\n"," [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n","  8.01252314e-04]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SzxjvD7knfwN"},"source":["**Kỳ vọng đầu ra**:\n","\n","<table style=\"width:60%\">\n","\n","     <tr> \n","       <td> **softmax(x)** </td> \n","       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n","    1.21052389e-04]\n"," [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n","    8.01252314e-04]]</td> \n","     </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"qyyp5Z4snfwN"},"source":["**Lưu ý**:\n","- Nếu bạn in shape của x_exp, x_sum và s ở trên và chạy lại cell đánh giá, bạn sẽ thấy rằng x_sum có shape (2,1) trong khi x_exp và s có shape (2,5). **x_exp/x_sum** hoạt động nhờ Python broadcasting.\n","\n","Xin chúc mừng! Bây giờ bạn đã hiểu khá tốt về numpy Python và đã triển khai một số hàm hữu ích mà bạn sẽ sử dụng trong Deep learning."]},{"cell_type":"markdown","metadata":{"id":"0dS4WXo0nfwO"},"source":["<font color='blue'>\n","\n","**Những điều bạn cần nhớ:**\n","- np.exp(x) hoạt động với bất kỳ np.array x nào và áp dụng hàm mũ cho mọi tọa độ\n","- hàm sigmoid và gradient của nó\n","- image2vector thường được sử dụng trong deep learning\n","- np.reshape được sử dụng rộng rãi. Trong tương lai, bạn sẽ thấy rằng việc giữ thẳng kích thước ma trận/vectơ sẽ giúp loại bỏ rất nhiều lỗi\n","- numpy có các hàm tích hợp hiệu quả\n","- broadcasting cực kỳ hữu ích"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"UV1tMN6jnfwO"},"source":["## 2) Vectơ hóa"]},{"cell_type":"markdown","metadata":{"id":"wPlvKU_KnfwO"},"source":["\n","Trong deep learning, bạn phải xử lý các tập dữ liệu rất lớn. Do đó, một hàm không tối ưu về mặt tính toán có thể trở thành một nút thắt lớn trong thuật toán của bạn và có thể khiến mô hình mất nhiều thời gian để chạy. Để đảm bảo code của bạn hiệu quả về mặt tính toán, hãy sử dụng vectơ hóa. Ví dụ: cố gắng phân biệt sự khác biệt giữa các cách triển khai sau đây của tích vô hướng/tích ngoài/elementwise."]},{"cell_type":"code","metadata":{"id":"X2raElv3nfwP","outputId":"1f61d234-b46e-4117-cfce-17a0f160244c"},"source":["import time\n","\n","x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n","x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n","\n","### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n","tic = time.process_time()\n","dot = 0\n","for i in range(len(x1)):\n","    dot+= x1[i]*x2[i]\n","toc = time.process_time()\n","print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n","tic = time.process_time()\n","outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n","for i in range(len(x1)):\n","    for j in range(len(x2)):\n","        outer[i,j] = x1[i]*x2[j]\n","toc = time.process_time()\n","print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n","tic = time.process_time()\n","mul = np.zeros(len(x1))\n","for i in range(len(x1)):\n","    mul[i] = x1[i]*x2[i]\n","toc = time.process_time()\n","print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n","W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n","tic = time.process_time()\n","gdot = np.zeros(W.shape[0])\n","for i in range(W.shape[0]):\n","    for j in range(len(x1)):\n","        gdot[i] += W[i,j]*x1[j]\n","toc = time.process_time()\n","print (\"gdot = \" + str(gdot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dot = 278\n"," ----- Computation time = 0.0ms\n","outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n"," [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n"," [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n"," [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n"," [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n"," [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"," ----- Computation time = 0.0ms\n","elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n"," ----- Computation time = 0.0ms\n","gdot = [14.47805961 24.98903737 18.48375151]\n"," ----- Computation time = 0.0ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QsHOTMwfnfwQ","outputId":"2b1df529-12c0-44d0-ebb5-a026a3386212"},"source":["x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n","x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n","\n","### VECTORIZED DOT PRODUCT OF VECTORS ###\n","tic = time.process_time()\n","dot = np.dot(x1,x2)\n","toc = time.process_time()\n","print (\"dot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED OUTER PRODUCT ###\n","tic = time.process_time()\n","outer = np.outer(x1,x2)\n","toc = time.process_time()\n","print (\"outer = \" + str(outer) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n","tic = time.process_time()\n","mul = np.multiply(x1,x2)\n","toc = time.process_time()\n","print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")\n","\n","### VECTORIZED GENERAL DOT PRODUCT ###\n","tic = time.process_time()\n","dot = np.dot(W,x1)\n","toc = time.process_time()\n","print (\"gdot = \" + str(dot) + \"\\n ----- Computation time = \" + str(1000*(toc - tic)) + \"ms\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dot = 278\n"," ----- Computation time = 0.0ms\n","outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n"," [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n"," [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"," [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"," ----- Computation time = 0.0ms\n","elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n"," ----- Computation time = 0.0ms\n","gdot = [14.47805961 24.98903737 18.48375151]\n"," ----- Computation time = 15.625ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FpVb7BfxnfwS"},"source":["Như bạn có thể nhận thấy, việc triển khai vectơ hóa nhanh và hiệu quả hơn nhiều. Đối với các vectơ/ma trận lớn hơn, sự khác biệt về thời gian chạy thậm chí còn lớn hơn.\n","\n","**Lưu ý** rằng `np.dot()` thực hiện phép nhân ma trận-ma trận hoặc ma trận-vectơ. Điều này khác với toán tử `np.multiply()` và `*` (tương đương với `.*` trong Matlab/Octave), thực hiện phép nhân từng phần tử của 2 ma trận cùng chiều."]},{"cell_type":"markdown","metadata":{"id":"Yb_-hjmenfwS"},"source":["### 2.1 Thực hiện các hàm mất mát L1 và L2\n","\n","**Task 8**: Thực hiện phiên bản vectơ hóa numpy của hàm mất mát L1. Bạn có thể thấy hàm abs(x) (giá trị tuyệt đối của x) hữu ích.\n","\n","**Nhắc nhở**:\n","- Hàm mất mát được sử dụng để đánh giá hiệu suất mô hình của bạn. Giá trị mất mát càng lớn thì các dự đoán của bạn ($\\hat {y} $) càng khác với giá trị thực ($y$). Trong deep learning, bạn sử dụng các thuật toán tối ưu hóa như Gradient Descent để huấn luyện mô hình và để giảm thiểu chi phí.\n","- Giá trị mất mát L1 được xác định như sau:\n","$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"]},{"cell_type":"code","metadata":{"id":"_3GTw9gQnfwS"},"source":["def L1(yhat, y):\n","    \"\"\"\n","    Arguments:\n","    yhat -- vector of size m (predicted labels)\n","    y -- vector of size m (true labels)\n","    \n","    Returns:\n","    loss -- the value of the L1 loss function defined above\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","\n","    ### END CODE HERE ###\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzrOQ3axnfwV","outputId":"de53d655-1be2-430d-9d92-a92cc90bcebb"},"source":["yhat = np.array([.9, 0.2, 0.1, .4, .9])\n","y = np.array([1, 0, 0, 1, 1])\n","print(\"L1 = \" + str(L1(yhat,y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["L1 = 1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5o47CxuQnfwY"},"source":["**Kỳ vọng đầu ra**:\n","\n","<table style=\"width:20%\">\n","\n","     <tr> \n","       <td> **L1** </td> \n","       <td> 1.1 </td> \n","     </tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"2zSr1VhWnfwY"},"source":["**Task 9**: Thực hiện phiên bản vectơ hóa numpy của giá trị mất mát L2. Có một số cách để thực hiện L2 nhưng bạn sẽ thấy hàm np.dot() hữu ích. Xin nhắc lại, nếu $x = [x_1, x_2, ..., x_n]$, thì `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n","\n","- L2 được xác định như sau $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"]},{"cell_type":"code","metadata":{"id":"2e6_zQN7nfwZ"},"source":["def L2(yhat, y):\n","    \"\"\"\n","    Arguments:\n","    yhat -- vector of size m (predicted labels)\n","    y -- vector of size m (true labels)\n","    \n","    Returns:\n","    loss -- the value of the L2 loss function defined above\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    \n","    ### END CODE HERE ###\n","    \n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YF-2bBXqnfwa","outputId":"a82a8195-29d9-4ffd-9107-b09403503bc1"},"source":["yhat = np.array([.9, 0.2, 0.1, .4, .9])\n","y = np.array([1, 0, 0, 1, 1])\n","print(\"L2 = \" + str(L2(yhat,y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["L2 = 0.43\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xWbFk0klnfwc"},"source":["**Kỳ vọng đầu ra**: \n","<table style=\"width:20%\">\n","     <tr> \n","       <td> **L2** </td> \n","       <td> 0.43 </td> \n","     </tr>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"VPK3KRs7nfwd"},"source":["Chúc mừng bạn đã hoàn thành bài tập này. Chúng tôi hy vọng rằng bài tập khởi động nhỏ này sẽ giúp ích cho các bạn trong những bài tập sắp tới, trở nên hào hứng và thú vị hơn!"]},{"cell_type":"markdown","metadata":{"id":"V0TjDSkWnfwd"},"source":["<font color='blue'>\n","\n","**Những điều cần nhớ::**\n","- Vectơ hóa là rất quan trọng trong deep learning. Nó giúp tính toán hiệu suất và rõ ràng hơn.\n","- Bạn đã xem lại hàm mất mát L1 và L2.\n","- Bạn đã quen với nhiều hàm numpy như np.sum, np.dot, np.multiply, np.maximum, v.v."]},{"cell_type":"code","metadata":{"id":"2VceWrtG07Mo"},"source":[""],"execution_count":null,"outputs":[]}]}